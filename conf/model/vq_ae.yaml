defaults:
  - optim@optim_conf: adamw
  - loss_f@loss_f_conf: huber
  - encoder@encoder_conf: default
  - decoder@decoder_conf: default
  - metrics: null

  - optional optional_overrides/optim: ${model/optim@model.optim_conf}

  # Init arguments for pre_activation_fixup
  - optional optional_overrides/pre_activation_fixup@encoder_conf.down_block_conf.conv_conf: ${model/layers/conv_block@model.encoder_conf.down_block_conf.conv_conf}_down
  - optional optional_overrides/pre_activation_fixup@encoder_conf.conv_block_conf: ${model/layers/conv_block@model.encoder_conf.conv_block_conf}_pre_enc
  - optional optional_overrides/pre_activation_fixup@decoder_conf.conv_block_conf: ${model/layers/conv_block@model.decoder_conf.conv_block_conf}_post_enc
  - optional optional_overrides/pre_activation_fixup@decoder_conf.up_block_conf.conv_conf: ${model/layers/conv_block@model.decoder_conf.up_block_conf.conv_conf}_up

_target_: 'vq_ae.model.VQAE'
_recursive_: False # we init the optim locally

metrics:
  _target_: utils.instantiate_dictified_listconf

encoder_conf:
  stem_conf:
    in_channels: 3
    out_channels: 4
  down_block_conf:
    n_down: 3
    n_pre_layers: 1
    n_post_layers: 4
  n_pre_enc_layers: 150
  vq_conf:
    '0':
      embedding_dim: ${mul:${model.encoder_conf.stem_conf.out_channels},${pow:2,${model.encoder_conf.down_block_conf.n_down}}}

decoder_conf:
  n_enc_layers: ${len:${oc.dict.keys:model.encoder_conf.vq_conf}}
  stem_conf:
    in_channels: ${model.encoder_conf.stem_conf.out_channels}
    out_channels: ${model.encoder_conf.stem_conf.in_channels}
  n_post_enc_layers: 150
  up_block_conf:
    n_up: ${model.encoder_conf.down_block_conf.n_down}
    n_pre_layers: 1
    n_post_layers: 4
